{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named matplotlib.pyplot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7b7f04cc2c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../tools/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named matplotlib.pyplot"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "financial_features = ['deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', \n",
    "                      'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', \n",
    "                      'long_term_incentive', 'restricted_stock', 'director_fees'] \n",
    "email_features = ['to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', \n",
    "                  'shared_receipt_with_poi']\n",
    "\n",
    "#features_list = ['poi','salary'] # You will need to use more features\n",
    "# adding the financial & email features\n",
    "features_list = ['poi','salary'] + financial_features + email_features\n",
    "#print features_list\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# Convert to pandas dataframe and get some statistics\n",
    "all_features = ['salary','deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', \n",
    "        'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', \n",
    "        'restricted_stock', 'director_fees','to_messages', 'from_poi_to_this_person', 'from_messages', \n",
    "        'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "enron_pd = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "enron_pd[all_features] = enron_pd[all_features].astype(float)\n",
    "print enron_pd.describe()\n",
    "\n",
    "#how many POIs\n",
    "poi_count = 0\n",
    "for p in range(len(enron_pd)):\n",
    "    if enron_pd['poi'][p] == True:\n",
    "        poi_count += 1\n",
    "print \"There are\", poi_count, \"POI (persons of interest) and\", 146-poi_count, \"non-POI\"\n",
    "\n",
    "# REMOVE \"TOTAL\", \"THE TRAVEL AGENCY IN THE PARK\" rows\n",
    "# code from Lesson: Enron Outliers to plot\n",
    "data_out = featureFormat(data_dict, features_list)\n",
    "for point in data_out:\n",
    "    salary = point[1]\n",
    "    bonus = point[2]\n",
    "#matplotlib.pyplot.scatter( salary, bonus )\n",
    "    plt.scatter( salary, bonus )\n",
    "\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")\n",
    "plt.show()\n",
    "# remove the 2 outliers\n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", 1)\n",
    "### Task 3: Create new feature\n",
    "### Created a feature called poi_num_emails which is both emails from this person to\n",
    "### a POI or emails from a POI to this person\n",
    "for key in data_dict:\n",
    "    to_poi = data_dict[key]['from_this_person_to_poi']\n",
    "    from_poi = data_dict[key]['from_poi_to_this_person']\n",
    "    if to_poi == 'NaN' and from_poi == 'NaN':\n",
    "        data_dict[key]['poi_num_emails'] = 0\n",
    "    elif to_poi == 'NaN' and from_poi != 'NaN':\n",
    "        data_dict[key]['poi_num_emails'] = 0 + from_poi\n",
    "    elif to_poi != 'NaN' and from_poi == 'NaN':\n",
    "        data_dict[key]['poi_num_emails'] = to_poi + 0        \n",
    "    else:\n",
    "        data_dict[key]['poi_num_emails'] = to_poi + from_poi\n",
    "\n",
    "features_list = ['poi','salary'] + financial_features + email_features + ['poi_num_emails']\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a variety of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "# GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from time import time\n",
    "nb_clf = GaussianNB()\n",
    "\n",
    "t0=time()\n",
    "nb_clf.fit(features, labels)\n",
    "#print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "#print(nb_clf.predict(features_test))\n",
    "\n",
    "nb_accuracy = nb_clf.score(features, labels)\n",
    "print \"Naive Bayes Accuracy:\", nb_accuracy\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn import tree\n",
    "    \n",
    "#dt_clf = tree.DecisionTreeClassifier(min_samples_split=40)\n",
    "dt_clf = tree.DecisionTreeClassifier()\n",
    "    \n",
    "dt_clf = dt_clf.fit(features, labels)\n",
    "\n",
    "pred = dt_clf.predict(features)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "dt_acc = accuracy_score(pred, labels)\n",
    "print \"Decision Tree:\", dt_acc\n",
    "\n",
    "# K-means\n",
    "from sklearn.cluster import KMeans\n",
    "km_clf = KMeans(n_clusters=2)\n",
    "km_clf.fit(features)\n",
    "pred = km_clf.predict(features)\n",
    "km_acc = accuracy_score(pred, labels)\n",
    "print \"K-means:\", km_acc\n",
    "\n",
    "# Adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ab_clf = AdaBoostClassifier(n_estimators=100)\n",
    "ab_clf.fit(features, labels)\n",
    "pred = ab_clf.predict(features)\n",
    "ab_acc = accuracy_score(pred, labels)\n",
    "print \"Adaboost\", ab_acc\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=10)\n",
    "rf_clf.fit(features, labels)\n",
    "pred = rf_clf.predict(features)\n",
    "rf_acc = accuracy_score(pred, labels)\n",
    "print \"Random Forest\", rf_acc\n",
    "# the best one so far\n",
    "clf = dt_clf\n",
    "#########################################################\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Find the features with the highest scores\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn import cross_validation\n",
    "import numpy as np\n",
    "\n",
    "selector = SelectKBest(f_classif, k=\"all\")\n",
    "selector.fit_transform(features,labels)\n",
    "\n",
    "# Get the scores\n",
    "print selector.scores_\n",
    "print features_list\n",
    "\n",
    "## Plot SelectKBest features ##\n",
    "kfeatures_list = ['salary'] + financial_features + email_features + ['poi_num_emails']\n",
    "y = np.array(selector.scores_)\n",
    "\n",
    "kfeatures_scores_dict = dict(zip (kfeatures_list, y))\n",
    "kfeatures_scores = sorted(kfeatures_scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "kfeatures1 = []\n",
    "kscores1 = []\n",
    "kfeatures_values = []\n",
    "\n",
    "kf_count = 0\n",
    "for kfeats in kfeatures_scores:\n",
    "    kfeatures1.append(kfeatures_scores[kf_count][0])\n",
    "    kscores1.append(kfeatures_scores[kf_count][1])\n",
    "    kfeatures_values.append(kf_count)\n",
    "    kf_count += 1\n",
    "        \n",
    "# Now plot\n",
    "plt.figure().suptitle('Features by SelectKBest Scores', fontsize = 14)\n",
    "plt.xticks(kfeatures_values, kfeatures1, rotation=90)\n",
    "plt.bar(kfeatures_values, kscores1)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('SelectKBest Feature Scores')\n",
    "plt.show()\n",
    "\n",
    "# rerun with highest scores features\n",
    "# salary, bonus, total stock value, exercised stock options\n",
    "features_list = ['poi','salary', 'bonus', 'total_stock_value', 'exercised_stock_options']\n",
    "# if you run with the 8 highest scores for features + my poi_num_emails\n",
    "#features_list = ['poi','salary', 'bonus', 'total_stock_value', 'exercised_stock_options',\n",
    "#                'deferred_income','long_term_incentive','restricted_stock','shared_receipt_with_poi', 'poi_num_emails']\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a SVC classification model\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "param_grid = {\n",
    "         'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "          'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],\n",
    "          }\n",
    "\n",
    "grid_clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "grid_clf = grid_clf.fit(features_train, labels_train)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print grid_clf.best_estimator_\n",
    "\n",
    "# re-running with results of grid_clf.best_estimator\n",
    "svc_clf = SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma=0.0001, kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "t0=time()\n",
    "svc_clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "print(svc_clf.predict(features_train))\n",
    "pred = svc_clf.predict(features_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "svc_acc = accuracy_score(pred, labels_train)\n",
    "print svc_acc\n",
    "\n",
    "# Train a Decision Tree classification model\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import  cross_val_score\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "param_grid = {'max_depth': range(2, 7)}\n",
    "grid_clf_tree = GridSearchCV(tree.DecisionTreeClassifier(), param_grid)\n",
    "grid_clf_tree.fit(features_train, labels_train)\n",
    "\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print grid_clf_tree.best_estimator_\n",
    "\n",
    "## Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "rescaled_features = scaler.fit_transform(features_train)\n",
    "rescaled_features\n",
    "feature_scaling_clf = km_clf.fit(rescaled_features, labels_train)\n",
    "feature_scaling_pred = km_clf.predict(rescaled_features)\n",
    "\n",
    "feature_scaling_acc = accuracy_score(feature_scaling_pred, labels_train)\n",
    "print \"K-means with Feature Scaling:\", feature_scaling_acc\n",
    "\n",
    "# Training Adaboost\n",
    "from sklearn import grid_search\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "parameters = {'n_estimators' : [5, 10, 30, 40, 50, 100,150], 'learning_rate' : [0.1, 0.5, 1, 1.5, 2, 2.5], 'algorithm' : ('SAMME', 'SAMME.R')}\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=8))\n",
    "adaclf = grid_search.GridSearchCV(ada_clf, parameters)\n",
    "adaclf.fit(features, labels)\n",
    "print \"Adaboost\", adaclf.best_score_\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "# re-running with results of grid_clf_tree.best_estimator\n",
    "tree_clf = tree.DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='best')\n",
    "tree_clf = tree_clf.fit(features_train, labels_train)\n",
    "pred = tree_clf.predict(features_train)\n",
    "\n",
    "##  the best one now\n",
    "#clf = svc_clf\n",
    "#clf = tree_clf\n",
    "clf = dt_clf\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.78446\tPrecision: 0.30905\tRecall: 0.32450\tF1: 0.31659\tF2: 0.32129\n",
      "\tTotal predictions: 13000\tTrue positives:  649\tFalse positives: 1451\tFalse negatives: 1351\tTrue negatives: 9549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        pred = clf.predict(features_test)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            else:\n",
    "                true_positives += 1\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    pickle.dump(clf, open(CLF_PICKLE_FILENAME, \"w\") )\n",
    "    pickle.dump(dataset, open(DATASET_PICKLE_FILENAME, \"w\") )\n",
    "    pickle.dump(feature_list, open(FEATURE_LIST_FILENAME, \"w\") )\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    clf = pickle.load(open(CLF_PICKLE_FILENAME, \"r\") )\n",
    "    dataset = pickle.load(open(DATASET_PICKLE_FILENAME, \"r\") )\n",
    "    feature_list = pickle.load(open(FEATURE_LIST_FILENAME, \"r\"))\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
